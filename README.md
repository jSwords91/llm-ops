# **LLM-Ops**

I'll add to this over time.

Things such as reducing latency for open sourced self-hosted models, quantization, LORA, logging, bootstrapping datasets etc.

